#!/usr/bin/env python
# coding: utf-8

# # Johdanto datatieteeseen oppimispäiväkirja 

#                                                            

# ## Luentoviikko 1: Johdanto

# Johdanto datatieteeseen -opintojakson oppimispäiväkirjan tarkoituksena on mahdollistaa opiskelijoille luentoviikoilla käsiteltyjen asioiden oma työstäminen ja tätä kautta syvällisempi ymmärtäminen sekä omaksuminen. Oppimispäiväkirja pohjautuu luentoihin, demosessioihin sekä niiden työkirjoihin. Lisäksi kuitenkin päiväkirjan eri osioissa hyödynnetään erinäisiä ulkopuolisia lähteitä, jotka ovat listattuna tiedoston lopussa. Opintojakso käsittelee nimensä mukaisesti datatiedettä ja siihen kuuluvia alueita kuten datan analysointia, visualisointia sekä ohjelmointia. Itseltäni ei löydy aiempaa kokemusta datatieteilystä, joten uskon opintojakson olevan erinomainen keino lähteä liikeelle aihealueen parissa. Kurssin lopuksi toivon ymmärtäväni paremmin mitä kaikkia aihealueita datatieteeseen sisältyy, kuinka nämä aihealueet suhtautuvat toisiinsa nähden sekä kuinka datatiede näkyy yritysten toiminnassa. Lisäksi toivon tietenkin kehittyväni ohjelmoinnin parissa. 

# Ensimmäisellä luentoviikolla polkaistiin sekä luentojen että demosessioiden osalta opintojakso käyntiin. Kurssin alku oli minulle jännittävä, sillä kurssin ensimmäinen luento oli minulle ihka ensimmäinen liveluento yliopistourallani. Oloni ennen  luentoa oli kuin ensimmäisenä koulupäivänä ja vatsassakin tuntui perhosia. Luennolla istuminen tuntuikin niin kummalliselta, että itse opetukseen keskittyminen ei oikein onnistunut ja päädyin tätä oppimispäiväkirjan osiota kirjoittaessani turvautumaan myös luennon tallennuksiin. Demosessiolle en osallistunut paikan päällä vaan katsoin kyseisen oppimistilaisuuden tallenteen. 

# Kyseisen viikon luennon painopiste oli opintojakson yleisten informaatioiden ja käytäntöjen läpikäynnissä, joiden jälkeen kuitenkin päästiin itse asiaan. Opintojaksolla liikutaan tietojohtamisen aihealueella ja datan saamisen sekä tutkimisen lisäksi pohditaan, kuinka datasta tuotetaan informaatiota, tietämystä ja tätä korkeampia tiedon tasoja kuten viisautta. Tämän toimenpiteen saavuttamiseksi tarvitaan ammattitaitoista datatieteilijää. Tähän ammattitaitoon liitetään yleisesti tilastolliset taidot, koneoppimisen hallinta, ohjelmointiosaaminen, visualisointitaidot sekä liiketoimintaosaaminen. Verrattuna 'datainsinööreihin' datatieteilijöiden pääpainopiste on uuden luomisessa ja improvisaatiossa kun taas insinöörien painopiste on standardoiduissa toimenpiteissä sekä tiedon jalostamisessa rutiininomaisesti. Datatieteessä siis luodaan uutta ja jalostetaan tietoa kysymällä kysymyksiä datalta rikkoen samalla koneiden ja ohjelmistojen asettamia rajoituksia. Näiden edellä esitettyjen tietojen valossa, itse näen datatietelijät luovina tekijöinä, jotka auttavat organisaatiossa sekä tiedon johtamisessa että tiedolla johtamisessa. Tämän takia uskon kyseisen opintojakson ja datatieteen osaamisen ylipäätään olevan erinomainen lisäase työmarkkinoilla. Datan jalostamisen pääpiirteet ovat minulle jo entuudestaan tuttuja esimerkiksi Tietojohtamisen perusteet -kurssilta ja liiketoimintatietämystä minulta löytyy esimerkiksi laskentatoimen kurssien sekä Liiketoimintatiedon hallinta -kurssin ansiosta. Näiden lisäksi myös ohjelmointitaitoja on hieman ehtinyt kertyä. 

# Luennolla esitettiin, kuinka datatieteeseen sisältyy osana tilastotiede sekä kuinka näissä kahdessa käyettävät ohejlmistot poikkeavat toisitaan Datatieteessä suosittuna ohjelmointikielenä toimii nykypäivänä Python ja se on kasvattanut suosiotaan tilastotieteessä käytettävän R-kielen ohi. Itse olen huomannut tämän suosion siinä, kuinka paljon kursseilla hyödynnetään Pythonia kun taas R-kielestä olen kuullut vain ohimennen joillakin matematiikan luennoilla. Lisäksi olen kuullut, että nykyisessä lukion opetussuunnitelmassa pakollisessa ohjelmointikurssissakin kielenä käytetään Pythonia. Näkisin tämän suosion kasvun johtuvan nimenomaan siitä, että nykypäivän organisaatioissa arvotetaan enenevissä määrin tietojohtamista, jolloin myös hyödynnettävältä datalta vaaditaan enemmän. Vaatimuksien täyttämiseksi tarvitaan ohjelmisto, joka taipuu tilastollisen laskemisen ja grafiikan tuottamisen lisäksi myös datan jalostamiseen sekä koneoppimiseen. Toimivan ohjelmiston lisäksi informaation ja tietämyksen saamisen varmistamiseksi tietenkin tarvitaan lahjakas datatieteilijä toteuttamaan datan jalostus. Luennolla esitettiin muutamia malleja datatieteestä, joille kaikille yhteistä oli liiketoiminnan ymmärrys ja osaaminen. Tämä on tärkeää, jotta datalta osataan kysyä oikeita kysymyksiä, jotka hyödyttävät organisaatiota ja auttavat sen toiminnan kehittämisessä. Näiden kysymysten löytämisessä näkisin tärkeänä myös School of data science Amsterdamin (2017) esittämät pehmeät taidot kuten intohimon ja aidon kiinnostuksen aihetta kohtaan sekä ongelmanratkaisukyvyn, sillä data ei itsessään osaa kertoa asioita ellei sitä ole tulkitsemassa ja siivoamassa datasta itsestään sekä organisaation liiketoiminnasta tietävä henkilö.

# Viikon demosessiossa lähdettiin liikkeelle aivan datatieteen alkeista sekä itse ohjelmistojen lataamisesta. Session aluksi esiteltiin Jupyterin avaaminen pip:iä hyödyntämällä, mutta tämä oli itselleni täysin hepreaa, joten päädyin lataamaan Anacondan ja avaamaan Jupyterin sen avulla. Jupyter itsessään on selkeä työalusta ja mahdollisten virheilmoitusten ratkaiseminen on helppoa. Tämän viikon demokoodissa paneuduin harjoittelemaan datan lukemista, sen tutkimista sekä hieman sen ominaisuuksien muokkaamista. Datana käytin Avoin datan (2018) tiedostoa väistötiloissa opiskelevista peruskoululaisista syyslukukaudella 2018. Kyseinen demo on esitetty alla.

#                                         

# In[16]:


# Tuodaan käytettävät kirjastot
import pandas as pd

# Määritetään datan osoite ja luetaan se pandasia käyttäen
url = 'http://www.avoindata.fi/data/dataset/637162ba-75f0-4d0a-81cd-4a21c17e44bd/resource/11e1d4b7-6a06-4c8a-a504-6ccede3a786d/download/ilmoitetut_koulut_ja_syytiedot.csv'
orig_df = pd.read_csv(url)

# Tulostetaan dataframe
orig_df


# In[17]:


# Tulostetaan sarakkeiden nimet
print('Sarakkeet:', orig_df.columns.values)


# In[18]:


# Tulostetaan datatyypit
print(orig_df.dtypes)


# In[19]:


# Treenataan funktion luomista
def caps(x):
    return x.upper()

# Muutetaan erään taulun sarakkeen kirjoitustyyli funktiota hyödyntäen
orig_df['Maakunta'] = orig_df['Maakunta'].apply(caps)


# In[20]:


# Tulostetaan päivitetty dataframe ja huomataan muutokset
orig_df


#                                                                   

# Näen esittämäni koodin kuvastavan viikolla oppimaani, sillä datatieteilijälle erittäin olennainen taito on datan tuominen ohjelmistoon ja sen tutkiminen ohjelmiston ominaisuuksia hyödyntämällä.

# Tämän viikon luennon tärkein anti oli tiedot kurssin käytännöistä, sillä ilman niitä voisi suorittaminen olla hieman haastavaa. Itse aihealueesta opin datatieteen eri aihealueet sekä sen, millaisia osaamisia ammattitaitoiselta datatieteilijältä odotetaan. Lisäksi opin eron datatieteilijän ja datainsinöörin toimintatavoissa, joiden olin aiemmin luullut tarkoittavan käytännössä samaa asiaa. Viidentenä asiana opin, kuinka csv-muotoista dataa luetaan ja, kuinka sitä voidaan yksinkertaisin menetelmin tutkia Pythonilla. Kehitysvinkkinä luentoviikolle suosittelisin luennoitsijaa perehtymään luennon tallentamiseen kyseisen toteutuksen luentosalissa etukäteen, jotta sekä naamakamera että taulun kuva saataisiin nauhalle heti alusta asti. Tämä helpottaisi luennon seuraamista kotoa käsin. Toisaalta luennoitsijan esiintymisote oli miellyttävä ja opetusta oli helppo seurata.

#                                         

# ## Luentoviikko 2: Datan kerääminen ja jalostaminen

# Toinen luentoviikko käsitteli datan keräämistä ja sen jalostamista. Itse luennolla olin kyllä paikalla, mutta kahden vuoden etäopiskelun jäljiltä on kesittymiskykyni kuin kultakalalla ja jouduin jälleen oppimispäiväkirjan kirjoittamisessa turvautumaan luennon tallenteeseen. Demosessiolle en osallistunut vaan katsoin tapahtuman tallenteen.

# Datatiedeprosessi lähtee CRISP-DM-mallin mukaisesti liikkeelle liiketoimintakysymyksestä tai organisaation toimintaan liittyvästä kysymyksestä. Tämä siis poikkeaa kurssin käsittelyjärjestyksestä, jonka mukaan prosessi käynnistyisi datan keräämisellä, minkä jälkeen vasta pohdittaisiin mitä kyseiseltä datalta halutaan kysyä. Verrattaessa tätä kurssilla suoritettavaan harjoitustyöhön, näkisin CRISP-DM-mallin mukaisen lähestymistavan olevan oman työn suorittamisessa ideaalimpi, mutta koska en vielä omaa oikeastaan yhtään kokemusta kyseisen tyyppisestä työstä, luulen joutuvani pohtimaan tutkimuskysymystä vasta dataan tutustumisen jälkeen. Tällöin harjoitustyön suoritusjärjestys vastaa enemmänkin kurssin luentojen läpikäyntijärjestystä.

# Datan keräämisessä dataa hankitaan ostamalla, lataamalla tai keräämällä joko organisaation sisältä tai ulkoisista lähteistä. Luennoitsija esitti luennolla väitteen, ettei varsinaista raakadataa oikeasti edes ole olemassa, sillä datan kerääjä vie dataan aina omaa maailmankuvaa ja keräämisprosessissa henkilöiden välisen yhteistyön ja jalostumisen kautta datan tuoma merkitys muokkaantuu. Luentoviikon 1 aiheeseen peilaten näkisin tämän väitteen osaltaan liittyvän datatietelijän liiketoimintaosaamiseen ja sen ymmärtämiseen, sillä uskoisin organisaatioiden haluavan kerätä vain sellaista dataa, joka on heille hyödyllistä. Tällöin organisaation liiketoiminnallisia tavotteita ymmärtävä datatieteilijä osaa jo alussa kerätä vain sellaista dataa, jolla saavutetaan jonkin tason arvoa eikä tällöin data ole missään vaiheessa 'raakaa'.

# Eräitä keinoja datan keräämiselle ovat niin sanotut raapijat ja ryömijät, joiden avulla dataa voidaan kerätä esimerkiksi joltakin tietyltä nettisivustolta. Nämä ovat hyödyllisiä työkaluja, sillä niiden avulla voidaan kerätä lähes millaista dataa tahansa. Datatieteessä kuitenkin voidaan hyödyntää mitä tahansa dataa. Täytyy kuitenkin muistaa, ettei dataa välttämättä tarvitse olla paljoa, jotta se olisi hyödyllistä vaan usein hyödyn näkökulmasta laatu ja täsmällisyys ajavat määrän yli. Jälleen näkisin tähän voivan liittää edellä mainitsemani organisaatioiden tavoitteiden ja kerätyn datan yhteensopivuuden merkityksen, sillä datan kerääminen ja käsittely vie organisaatiolta aina resursseja, jolloin organisaation toiminnan kannalta turhan datan jalostamiseen kuluisi resursseja turhaan.

# Internetin hakukoneet toimivat raapijoina ja ryömijöinä. Esimerkiksi Googlen tulee jollakin tavalla selvittää, mistä eri nettisivut löytyvät ja tähän tarkoitukseen sen hakurobotin täytyy raapia Internetin ihmeellistä maailmaa. Vaikka tästä ylivaltijas Googlen ryömimisestä voisi saada kuvan, että lähes kaikkea voidaan raapia, ei tämä kuitenkaan ole totta vaan joillakin sivustoilla kuten esimerkiksi Facebookilla tai Instagramilla on sivuston koodiin kirjattuna ryöminnän estot. Ryömijän luomisessa on siis aina pidettävä mielessä, että kohtelias ryömijä ei ryömi sellaisia asioita jota se ei saa ryömiä. Tähän ryöminnän sallittavuuteen vaikuttavat sekä laillisuus että etiikka. Luennolla esitettiin myös ryömityn datan hyödyntäminen tutkimuksessa ja, kuinka raavinnalta kielletyillä sivustoilla dataa ei saa hyödyntää edes tutkimuksessa. Tämä herätti pohtimaan, olisiko tällaisen sivuston raavinta kuitenkin mahdollista sivustonhaltijan luvalla, jos tutkimuksessa saatavat tulokset mahdollisesti hyödyttäisivät tutkijan lisäksi myös haltijaa itseään?

# Datan keräämisen jälkeen täytyy dataa analysoida. Tähän on olemassa useampia erilaisia analyysityyppejä. Luentomateriaaleissa esitetyt tyypit ovat kuvaileva, diagnosoiva, ennakoiva sekä ohjaava analytiikka. Ennakoivassa analytiikassa pohditaan, mitä todennäköisesti on tapahtumassa. Tällainen pohdittava asia voi esimerkiksi olla mahdollinen poistuva asiakas, jolloin tämä poistuminen voidaan yrittää estää ennalta. Ohjailevassa analytiikassa pyritään aktiivisesti ohjaamaan esimerkiksi asiakkaan toimintaa. Diagnosoivassa analytiikassa pohditaan miksi jokin asia tapahtui tai esimerkiksi koneoppimisen kautta saadusta ennusteesta pyritään pohtia, miksi ennusteen mukainen asia tulee mahdollisesti tapahtumaan. Kuvailevassa analytiikassa sen sijaan pohditaan mitä tapahtui. Luennoitsija itse mielellään käyttää eri analytiikoissa jakoa eksploratiivinen, deskriptiivinen, prediktiivinen, preskriptiivinen sekä ohjaileva. Kaikissa eri analytiikkamalleissa tarvitaan dataa. Näkisin itse analytiikan olevan lähes tärkeämpi osa data-analyysia datan keräämisen laadun sijasta, sillä laadukkaalla analyysilla ja eri analytiikkatyyppejä yhdistelemällä voidaan datasta löytää yllättäviäkin informaatioita organisaation toiminnasta.

# Luentoviikon ja demosession sisältöjen painottuessa datan keräämiseen raapijoiden ja ryömijöiden avulla, päätin itsekkin hieman kokeilla sellaisen alun rakentamista. Alla olevassa demokoodissa esittelen, kuinka raapijan rakentaminen alustetaan ja kuinka tiedot haetaan halutulta nettisivustolta. Varsinaista raapijan koodiosiota en demoon alkanut väsäämään.

#                                      

# In[22]:


# Asennetaan scrapy
get_ipython().system('pip install scrapy')


# In[27]:


# Luodaan scrapyn avulla automaattisesti raapijaa varten tarvittavan koodin rungon
get_ipython().system('scrapy genspider gigantti_scraper gigantti.fi')


# In[33]:


# Luodaan uusi python-script, gigantti_scraper.py-tiedosto. Tämän jälkeen tulisi varsinainen koodi, 
# mutta demon pituuden puitteissa en alkanut esittää sitä nyt.
import scrapy
class GiganttiScraperSpider(scrapy.Spider):
    name = 'gigantti_scraper'
    allowed_domains = ['gigantti.fi']
    # Tuotteen arvostelujen url-osoite
    start_urls = ['https://www.gigantti.fi/product/kodin-pienkoneet/grillit/kaasugrillit/weber-genesis-ii-e310-kaasugrilli-61010169-musta/17523#reviews']


# In[35]:


# Kutsutaan scriptiä (päiväkirjan siisteyden vuoksi en aja tätä)
get_ipython().system('scrapy runspider gigantti_scraper.py -o out.json')


#                                     

# Luentoviikon keskeisimmät opit olivat raapijoihin liittyvät tiedot siitä, mitä ne käytännössä ovat ja, kuinka niitä voidaan luoda. Tämä raapijoiden rakentaminen kuitenkin vaatii vielä harjoitusta, jotta sellaisen luominen kulkisi sujuvasti. Kolmanneksi opin, että internetin raapimista rajoitetaan aktiivisesti sekä itse sivustojen että lainsäädännön puolesta. Neljäntenä opin lisää Pythonin datatyypeistä, minkä uskon olevan hyödyllistä tietoa harjoitustyön suorittamisen kannalta. Viimeiseksi mieleen jäi, kuinka datan jalostaminen vaatii aina resursseja, jolloin keräämistä ja sen tärkeyttä tulee harkita organisaatioissa. Kehitysvinkkinä luentoviikolle esittäisin raapijan rakentamisen rautalankamallin esittämistä, sillä luento- ja demomateriaaleissa esitetyt esimerkit jäivät ainakin itselle vielä hieman epäselviksi.

#                                             

# ## Luentoviikko 3: Koneoppimisen perusteet 

# Opintojakson kolmannella luentoviikolla sukellettiin koneoppimisen kiehtovaan maailmaan. Jälleen kerran olin liveluennolla paikalla, mutta oppimispäiväkirjaa kirjoittaessani päädyin hyödyntämään tallennetta muistin virkistämiseksi. Demosessiosta katsoin jälleen pelkän tallenteen.

# Datatieteessä koneoppimisella voidaan opettaa jokin algoritmi ennustamaan datasta jotakin asioita datan piirteisiin perustuen jonkin muuttujan suhteen. Piirteiden erottamsiella tarkoitetaan sellaisen joukon muuttujia erottamista aineistosta, joiden ajatellaan ennustavan ennustettavaa muuttujaa. Muita syitä piirteiden erottamiselle ovat esimerkiksi tutkittavan tietomäärän vähentäminen, piirteiden määrän pienentäminen, suorituskyvyn parantaminen sekä tietojen kuvaileminen tai visualisointi. Itse näkisin näistä omille aloittelen datatieteilijän taidoille olennaisimmaksi syyksi tietomäärän vähentämisen, sillä luulen pienemmän datamäärän käsittelyn olevan yksinkertaisempaa. Löydetyistä piirteistä muodostuu piirrematriisi, jonka avulla voidaan toteuttaa koneoppimismalli. 

# Koneoppiminen voidaan jaotella erilaisiin tyyppeihin, joita ovat ohjattu oppiminen, ohjaamaton oppiminen, vahvistettu oppiminen sekä syväoppiminen. Itselle näiden tyyppien konkreettiset erot jäivät hieman epäselviksi, mutta uskon niiden selkeytyvän vielä tulevilla luentokerroilla. Ainakin ohjatun oppimisen uskon tulevan tutuksi harjoitustyön parissa, sillä harjoitustyössä mahdollisesti tulen tekemään lineaarisen regression, joka on luennoitsijan mukaan yksinkertainen ohjatun oppimisen menetelmä. Tutkimuskysymykseen sopivan mallin valitseminen voi tapahtua joko kokeilemalla tai puhtaasti analyysin toteuttajan kokemuksen ja ammattitaidon avulla. Koneoppimisen avulla datatieteessä voidaan myös jalostaa dataa lisäämällä siihen piirteitä. Sopivan mallin löydyttyä voidaan datapakettiin lisätä uutta tietoa, jonka lopputulos voidaan koneoppimisen avulla ennustaa. Toisaalta koneoppiminen ei jokaisessa kysymyksessä ole paras mahdollinen vaihtoehto vaan ongelmissa joissa tavoitteena on ymmärtää ja kuvailla aineistoa, on hyödyllisempää käyttää tilastollisia menetelmiä.

# Koneoppimiseen liittyy aina myös virheen mahdollisuus, jota voidaan pienentää jalostamalla piirteitä paremmin ja yhdistelemällä niitä siten, että saadaan suhdelukuja. Tästä inspiroituneena päädyin viikon demokoodissani harjoittelemaan hieman piirteiden erottelua sekä tunnuslukujen laskemista. Demossa datana toimii Avoin datan (2022a) tiedosto Kelan etuuksien saajista ja maksetuista etuuksista. Koodi on esitettynä alla. 

#                            

# In[41]:


# Määritetään url-osoite ja luetaan tiedosto
url1 = 'http://www.avoindata.fi/data/dataset/9af378a3-c235-491d-95fb-7226786250d0/resource/a3be1d46-1c33-4a59-84d4-66a348b1551c/download/data.csv'
df1 = pd.read_csv(url1)

# Tulostetaan dataframen tiedot
df1.head()


# In[42]:


# Poistetaan dataframesta joitakin sarakkeita
poistetaan = ['aikatyyppi', 'vuosikuukausi','kunta_nro','kunta_nimi','etuus']
data = df1.drop(poistetaan, axis=1)
data


# In[44]:


# Kuvaillaan dataa
data.describe()


# In[49]:


# Lasketaan joitakin kuvailuun liittyviä arvoja 'käsin' ja tulostetaan arvot
keskiarvo = np.mean(data, axis=0)
keskihajonta = np.std(data, axis=0)
normalisoitu = (data - keskiarvo) / keskihajonta
print('keskiarvo:')
print(keskiarvo)
print()
print('keskihajonta:')
print(keskihajonta)
print()
print('normalisoitu:')
print(normalisoitu)


#                                                         

# Tällä luentoviikolla opin, mitä mitä koneoppiminen datatieteessä on, mitä eroa koneoppimisella ja tekoälyllä on, mitä piirteillä ja niiden erottamisella tarkoitetaan datatieteessä sekä mitä eri syitä piirteiden erottamiselle on. Lisäksi koodidemoa kasatessani opin uusia komentoja datan käsittelyyn. Vinkkinä luennoitsijalle olisi, että luennon rakenteen voisi pyrkiä rakentamaan hieman selkeämmäksi, sillä ainakin itselläni oli ajoittain haasteita pysyä aiheessa mukana. Lisäksi esimerkkikoodien läpikäynnin voisi säästää kooditorioon.

#                                                    

# ## Luentoviikko 4: Harjoitustyöhön syventyminen 

# Neljännellä luentoviikolla paneuduttiin syvemmälle kurssilla suoritettavaan harjoitustyöhön ja lähdettiin pohtimaan, kuinka työ käytännössä kannattaisi suorittaa ja mitä siinä kannattaisi tutkia. Vaikka liveluennolta olikin tarjolla porkkanapiste, en saanut itseäni raahattua kampukselle, joten päiväkirjan kirjoittamiselle pohjan antoi luentotallenne sekä sen materiaalit. Demosessiosta katsoin pelkän tallenteen.

# Harjoitustyön suorittamisessa tulee lähteä liikkeelle käytettävän prosessin valinnasta. Luentomateriaaleissa esitetään, kuinka harjoitustyössä, kuten kaikissa datatiedeprojekteissa, tulisi lähteä liikkeelle liiketoimintakysymyksestä ja vasta tämän jälkeen kerätä data, jalostaa sitä ja analysoida sen piirteitä esimerkiksi koneoppimisen avulla. Tätä mallia edustavat jo aiemmilla luentokerroilla esille tullut CRISP-DM-malli sekä niin sanottu mahdollisuus-kanvaasi. Tästä luennoitsijan pienestä painostuksesta huolimatta itse kuitenkin uskon edelleen tekeväni työn luentojen mukaisella järjetyksellä ja miettiä tutkimuskysymystä vasta työn edetessä ja datan ollessa jo olemassa. 

# Luennolla porkkanapisteiden edellytyksenä oli osallistuminen pienryhmäkeskusteluihin, joissa pohdittiin itse AirBnB:n toimintaan liittyviä kysymyksiä sekä myös harjoitustyön suorittamiseen liittyviä kysymyksiä kuten esimerkiksi sitä, millaisia asioita tutkittavasta datasta voitaisiin mahdollisesti selvittää. Näistä mahdollisista tutkimuskysymyksistä selkeimmin esiin nousivat eri alueille mahdollisesti kuuluvat vaatimukset mukavuuksiin liittyen sekä asiakkaiden segmentointiin liittyvät kysymykset. Itse ajattelen keskittyväni työssä johonkin yksinkertaiseen numeeriseen dataan, jottei datan jalostamiseen ja muokkaamiseen tuhlaannu kovasti aikaa.

# Oppimistilaisuudessa pohdittiin tutkimusongelmaa myös yleistä tasoa laajemmin hyödyntäen edellä mainittua mahdollisuus-kanvaasia. Siinä pohdittiin ongelman lisäksi esimerkiksi sen asiakkaita, käyttäjiä, mahdollisia ratkaisuja sekä lyhyen ja pitkän aikavälin tarpeita. Itse näen esimerkiksi projektissa erilaisten mahdollisten asiakassegmenttien olevan itse yritys, hostit sekä asukkaat, joista jokainen voi hyötyä datasta ja sen tutkimiseta hieman eri tavoin. Esimerkiksi yritys voi hyötyä asuntojen hintakehityksestä kertovasta analyysistä, hostit erilaisia mukavuuksia analysoivasta projektista ja asiakkaat taas eri asuinalueista ja niiden palveluista kertovasta datasta. Projektini mahdollinen asiakaskunta on näillä tiedoilla todennäköisesti hostit, sillä näen tämän antavan suurimmat mahdollisuudet datan analysointiin. 

# Tämän viikon koodidemossa halusin keskittyä asiaan, josta voisin hyötyä harjoitustyötä tehdessä. Eräs harjoitustyössä toteutettava ominaisuus on koneoppimismalli, joista yksinkertaisin vaihtoehto on lineaarinen regressio. Tätä aihetta käsiteltiin myös viikon demosessiossa. Alla olevassa koodissa tutkin Avoin datan (2022b) dataa Kelan maksetuista takuueläkkeistä ja luon lineaarisen regression kuukauden vaikutuksesta maksettuihin etuuksiin.

#                                      

# In[37]:


# Määritetään halutun datan url-osoite ja luetaan se pandasilla
url2 = 'http://www.avoindata.fi/data/dataset/17cd6586-550b-4994-8b0d-806b2b0a3b59/resource/bc9a6486-3f0c-40cc-8016-8b7e803075b8/download/data.csv'
df2 = pd.read_csv(url2)

# Tulostetaan sarakkeiden nimet ja niiden tietotyypit
print(df2.info())


# In[74]:


# Määritetään käytettäviä kirjastoja
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model

# Luodaan lineaarinen regressio
linear_regression = linear_model.LinearRegression()
x = df2['kuukausi_nro'].values[:, np.newaxis]
y = df2['maksettu_eur']

classifier = linear_regression.fit(x,y)

plt.scatter(x,y, color='pink')
plt.plot(x, classifier.predict(x), color='black')
plt.title('Kuukauden vaikutus maksettuihin etuuksiin')
plt.xlabel('kuukausi_nro')
plt.ylabel('maksettu_eur')


#                                     

# Yllä olevasta kuvaajasta nähdään, kuinka etuuksia maksetaan muina kuukausina tasaisemmin, mutta joulukuussa maksujen suuruudessa tapahtuu jostakin syystä piikki. Lineaarinen regressio sen sijaan on vaakasuora, hoten tutkittavat muuttujat eivät ole vaikutuksessa toisiinsa.

# Tällä viikolla opittujen asioiden kirjaaminen oli luennon luonteesta johtuen hiukan haastavaa. Tästä huolimatta opin kuitenkin paremmin, kuinka lineaarinen regressio tehdään, minkä uskon hyödyttävän minua harjoitustyön tekemisessä. Lisäksi luennon ansiosta harjoitustyön tarkoitus on päässäni selkeämpi kuin ennen ja oppimistilaisuudessa pohditut kysymykset pakottivat hieman miettimään oman työn aloittamista sekä sen mahdollista tutkimuskysymystä. Kyseistä luentoviikkoa voisi mielestäni parantaa siten, että luento korvattaisiin työpajan tyyppisellä ratkaisulla, jossa jokainen voisi konkreettisesti saada omaa työtään eteenpäin tai edes alulle. Toinen parannusehdotus olisi se, että ennen oppimistilaisuutta jokainen voisi pohtia AirBnB:n artikkeliin liittyviä kysymyksiä, jolloin nämä vastaukset voitaisiin käydä oppimistilaisuuden aluksi läpi ja loppuaika voitaisiin pyhittää konkreettisesti itse harjoitustyölle.

#                                                 

# ## Luentoviikko 5 : Vierailuluento luonnollisen kielen analyysistä

# Viidennellä luentoviikolla me opiskelijat saimme iloksemme vierailevan luennoitsijan, kun Futuricen datainsinööri Teemu Mikkonen piti informatiivisen luennon luonnollisen kielen analyysistä. Olin itsekkin virtuaaliluennolla paikalla, mutta näppäränä tyttönä unohdin muistiinpanojen tekemisen, joten tämä oppimispäiväkirjan luku perustuu pääasiassa viime vuoden tallenteeseen. Demosessiosta katsoin vain tallenteen.

# Vuoden aikana Mikkonen oli onnistunut vaihtamaan Futuricelle Solitalta, joten viime vuoden luento käsitteli analyysiä Solitan näkövinkkelistä. Solita on vuonna 1996 Hervannassa perustettu yritys, joka toimii konsultoinnin, palvelumuotoilun, ohjelmistokehityksen, analytiikan, pilvipalveluiden sekä integraation parissa Suomessa, Ruotsissa, Tanskassa, Virossa, Belgiassa sekä Saksassa. Yritys on tunnettu muun muassa Koronavilkku- ja Omaolo-sovelluksista. Mikkonen itse on kotoisin Tampereelta ja on tietojohtamisen alumni. Hän aloitti opintonsa Tampereen teknillisessä yliopistossa vuonna 2014 ja valmistui Diplomi-insinööriksi Tampereen yliopistosta vuonna 2020. Datan parissa hän on työskennellyt CGI:llä, Solitalla ja nyt vuonna 2022 Futuricella. Mikkosen diplomityö liittyi nimenomaan luonnollisen kielen analyysiin.

# Luonnollisen kielen analyysi ei ollut minulle entuudestaan tuttua, joten luennon seuraaminen oli mielenkiintoista, mutta samalla myös haastavaa koodin monimutkaisuuden vuoksi. Luonnollisen kielen analyysi eli NLP on eräs datatieteiden erityisala. datalähteenä on luonnollinen kieli, jonka käsittelyyn vaaditaan erityistä osaamista, sillä sen rakenne asettaa haasteita siinä, kuinka sitä voidaan käyttää datalähteenä yleisempään numeeriseen dataan verrattuna. Keskeistä tässä on esikäsittely ja piirteiden erottaminen, joiden ansiosta päästään kosketuksiin datan numeerisen muodon kanssa ja erinäisten analyysien muodostaminen on helpompaa. Tavalliselle tallaajalle yleisimpiä NLP:n käyttökohteita ovat chatbotit ja automaattiset konekäännökset. Eli esimerkiksi Fortumin äärettömän kömpelö apubotti, joka ei itse osaa vastata asiakkaan vastauksiin vaan omalla kokemuksella lähes poikkeuksetta ohjaa asiakkaan asiakaspalvelun puolelle. Tämä kokemus huomioon ottaen en täysin ymmärrä, millaista arvoa NLP voi tuoda datan käsittelyyn, sillä mielestäni tekstipohjaisen datan käsittely vaikuttaa ensisilmäykseltä turhalta työltä. Esimerkiksi yritysmaailmassa tiedostojen kuten vaikka työhakemusten luokittelu NLP:n avulla tuntuu riskialttiilta, sillä uskoisin niiden läpikäynnin manuaalisesti ihmistyövoimalla olevan yrityksen arvojen ja hyödyn kannalta kannattavampaa. Näkisin, että tällöin voitaisiin olla lähes satavarmoja, ettei potentiaalisia hakijoita ole koneen toimesta hylätty ja, että firmaan saadaan rekrytoitua tehtävän kannalta juuri oikea henkilö.

# Luento itse painottui koodin läpikäyntiin ja ajamiseen. Koodissa ratkaistiin luokitteluongelmaa ministerien antamista vastauksista kirjallisiin kysymyksiin. Ongelman ratkaisu lähti aina tarvittavien kirjastojen lataamisesta, datan lukemiseen, hukkasanojen poistoon, esikäsittelyn kautta itse koneoppimismallin rakentamiseen ja tuloksiin. Omaan kokemattomaan silmääni suurimpana painotuksena koodissa esiintyi esikäsittelyn vaiheet, joihin kuuluivat alkukirjainten muuttaminen pieniksi, erikoismerkkien poisto, hukkasanojen poisto sekä perusmuotoistaminen. Vaikka tällä tavoin lueteltuna prosessi vaikuttaa ainakin omasta mielestäni loogiselta, ei se sitä koodia tuijottaessani ollut. Tästä voin kyllä täysin syyttää kokemattomuuttani ohjelmoinnin parissa.

# Oppimistilaisuuden koodi oli omaan silmään niin monimutkaista, etten demokoodia koostaessani uskaltanut lähteä vastaavanlaista koodia vääntämään. Täten alla oleva koodi on ote Mikkolan (2021) luentotyökirjasta kohdasta, jonka sisällön ymmärsin oma lähtötasoni huomioon ottaen itse lähes täysin. Mielestäni koodi kohtaa hyvin myös demosession sisältöön, joka myös käsitteli tekstin analysointia.

#                                                           

# In[54]:


# Ladataan tarvittava kirjasto
import re


# Määritetään muunnettava teksti (saatavilla Sisusta opintojakson kuvauksesta)
text = 'Datatiede rakentuu neljän laajan kokonaisuuden varaan: liiketoimintaosaaminen, ohjelmointi- ja tietokantaosaaminen, tilastollinen analyysi ja datalähtöinen viestintä ja visualisointi. Opintojaksolla on tavoitteena syventyä näihin aiheisiin datatieteen näkökulmasta sekä esitellä opiskelijoille riittävät tiedot datatiedeosaamiseen kuuluvien taitojen hankkimiseen Tampereen yliopiston opetustarjonnasta. Opintojakson suoritettuaan opiskelija osaa jäsentää datatieteen kentän keskeiset menetelmävaihtoehdot, arvioida menetelmien soveltumista tietojohtamisen ongelmakenttään ja toteuttaa yksinkertaisen analytiikkaprosessin laskennallisesti.'

# Tehdään tekstin sanoista lista
words = text.split(" ")

# Tyhjä lista esikäsitellyille sanoille
preprocessed_words = [] 

# Käsitellään sanat silmukalla
for word in words:
  word = re.sub("[^A-Za-z0-9ÄäÖö-]+", "", word) # Poistetaan regexin avulla kaikki merkit paitsi A-Ö-kirjaimet ja numerot
  word = re.sub("-", " ", word) # Muutetaan regex-komennon avulla kaikki väliviivalliset sanat eri sanoiksi vaihtamalla viivan tilalle välilyönti
  word = word.strip().lower() # Poistetaan sanan ympäriltä ylimääräiset välilyönnit ja muutetaan kaikki kirjaimet pieniksi
  if len(word) > 2:  # Hylätään kaikki kaksi merkkiä lyhyemmät merkkijonot
    preprocessed_words.append(word) # Lisätään sana esikäsiteltyjen sanojen listaan

# Liitetään listan sanat toisiinsa välilyönnein ja tulostetaan tekstit
text_sc = " ".join(preprocessed_words)
print("Alkuperäinen teksti: ", text)
print()
print("Erikoismerkit poistettu:", text_sc)


#                                                             

# Vaikka yllä esitetty koodi ei olekaan haastavuudeltaan päätä huimaava, niin uskon silti sen tason kuvastavan parhaiten omaa koodamistasoani tällä hetkellä. En myöskään usko kyseisen koodi pätkän edustavan NLP-prosessin tärkeimpiä vaiheita, sillä mielestäni parhaan arvon saavuttamiseksi datasta merkittävämpi vaihe prosessissa on arvottomien hukkasanojen poisto, jolloin data on siistimpää ja arvon hahmottaminen helpompaa.

# Mielestäni Mikkolan vierailuluento oli kokonaisuudessaan erittäin onnistunut ja herätti lisää kiinnostusta datatiedettä kohtaan sekä alana että jopa mahdollisesti urana. Uutena tietona minulle tuli, mitä kohina tarkoittaa tekstimuotoisessa datassa, kuinka konteksti vaikuttaa sanan arvoon NLP:stä puhuttaessa sekä, kuinka äärettömän tärkeä vaihe esikäsittely tekstimuotoisen datan kohdalla on. Lisäksi, ehkä luennon aiheesta hieman irrallisena huomiona, mieleeni selkeytyi paremmin numpyn toimintaperiaate. Luento oli mielestäni erinomainen läpileikkaus myös koneoppimisen prosessiin, joka tämän myötä selkeytyi mielessäni. Kehitysehdotuksia minulla ei vierailuluennolle ole muita kuin, että virtuaalitoteutuksen sijasta voisi olla mukavaa päästä kyseisen yrityksen tiloihin luennolle, sillä tällä tavoin olisi ehkä mahdollista saada vieläkin parempi kuva yrityksen toiminnasta, arvoista sekä ilmapiiristä. Tämä on tietenkin täysin riippuvaista yrityksestä ja heidän tiloistaan.

#                                                            

#                                     

# ## Luentoviikko 6: Ohjaamaton oppiminen 

# Toiseksi viimeisen luentoviikon aiheena toimi koneoppimisen aihealue ohjaamaton oppiminen. Oppimispäiväkirjan kirjoittaminen painottui täysin tallenteen varaan, sillä en taaskaan jaksanut hinata itseäni sähkötalon penkkiin. Demosessiosta yritin katsoa tallennetta, mutta ilmeisesti äänen tallentamisessa on tapahtunut joku virhe ja koko oppimistapahtumassa ei ollut saatavilla ääntä.

# Ohjaamaton oppiminen poikkeaa ohjatusta oppimisesta pääasiassa sen prosessin lopputuotteiden osalta. Molempien oppimistyyppien prosesseissa lähdetään liikeelle raakadatasta, josta erotetaan piirteitä, opetetaan malli, arvioidaan sitä ja saadaan datalle jokin ennuste. Ohjatun oppimisen kohdalla uuden datan tullessa oppimismalliin saadaan aina vain uusia labeleita ja niiden ennusteet. Sen sijaan ohjaamattomassa oppimisessa uuden datan tullessa koneeseen ei enää saadakkaan vain labeleita vaan tulokset voivat olla esimerkiksi klustereita kuten esimerkiksi asiakassegmenttejä tai jotain muita 'samanlaisia' datajoukkioita. Tämä klusterointi on erinomainen esimerkki ohjaamattomasta ohjauksesta. Lisäksi esimerkiksi ostoskorianalyysi on ohjaamatonta oppimista.

# Ostoskorianalyysista puhuttaessa mieleeni tuli ajatus siitä, että voitaisiinko ohjaamattomasta oppimisesta puhua, vaikka tiedon käsittelijänä ei olisikaan kone. Olen nimittäin itse työskennellyt useamman vuoden kaupan kassalla, jossa erilaisia ostoskoreja kantavista ihmisistä on muodostunut päähäni selkeät stereotyypit. Täten usein tuotteitta hyllyttäessäni kuvittelen millainen henkilö kyseistä tuotetta ostaa ja uuden tuntemattoman asiakkaan kävellessä kauppaan usein pohdin hänen ostavan todennäköisesti joitakin tiettyjä tuotteita. Toki välillä nämä stereotyypit rapistuvat yllättävien ostosten ilmestyessä ostoshihnalle, mutta kuitenkin suuressa kuvassa olen omassa päässäni muodostanut kuvat joukkioista, jotka tietyn tyyppisiä tuotteita ostavat ja usein nämä ajatusmallini osuvatkin oikeaan.

# Ryvästämisessä saadun datan pisteet jaetaan kokonaisuuksiksi siten, että kukin kokonaisuutensa sijaitsee mahdollisimman pienellä alueella, mutta kokonaisuudet itsessään sijaitsevat mahdollisimman kaukana toisistaan. Yleensä nämä kokonaisuudet omaavat jonkin yhteisen ominaisuuden, mutta saadussa tietojoukossa ei ole valmiina tietoa tästä yhteisestä tekijästä. Vaikka ryvästäminen onkin ohjaamattoman oppimisen menetelmä, tapahtuu siinäkin ohjaamista klustereiden määrää valittaessa. Tämän viikon demokoodissa halusin hieman tutustua ryvästämisen alkupään koodiin ja harjoitella tätä kautta lisää pandasin toimintoja kuten esimerkiksi tyhjäarvojen poistamista datan jalostamiseksi. Tämän uskon hyödyttävän suuresti harjoitustyötä tehdessä. Demokoodi on esitettynä alla. Koodissani tutkin Inside AirBnB:n (n.d.) datasettiä Amsterdamissa sijaitsevista asunnoista.

#                                                                      

# In[59]:


# Luetaan data
url3 = 'http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2022-03-08/visualisations/listings.csv'
df3 = pd.read_csv(url3)
df3


# In[60]:


# Luodaan uusi dataframe ja tulostetaan se
df4 = df3[['id','host_id','price','number_of_reviews','reviews_per_month','number_of_reviews_ltm','availability_365']]
df4.head()


# In[61]:


# Katsotaan kuinka paljon tyhjäarvoja dataframesta löytyy
df4.isnull().sum()


# In[62]:


# Muut ongelmat
tietueita = df4.shape
tietueita


# In[63]:


# Poistetaan tyhjäarvoja
dfNum=df4.dropna()
print("Poistettuja tietueita oli: "+str(tietueita[0]-df4.shape[0])+" jäljellä "+str(df4.shape[0]))


# In[64]:


# Piirretään histogrammit
hist = df4.hist()


#                                                               

# Demokoodissani esitetty ryvästäminen on vasta alkumaistiainen aiheeseen, sillä mielestäni tutkittavaa dataa tulisi jalostaa vieläkin pidemmälle. En ole myöskään täysin varma, kuinka tarkoituksenmukaisesti koodissani valitsin tutkittavat sarakkeet, sillä voisin kuvitella esimerkiksi kartan sijaintitietojen olevan kyseisen datan ryvästämisessä mahdollisesti hyödyllistä tietoa. 

# Ryvästämisen lisäksi eräs toinen ohjaamattoman oppimisen malli on aihemallinnus. Kyseisessä menetelmässä etsitään dokumenttien kokoelmassa esiintyviä abstrakteja aiheita ja tunnistetaan niitä. Keskeistä siinä on mahdollisten aiheiden löytäämisen todennäköisyyden kasvaminen useammasta dokumentista verrattuna vain yhden datalähteen tutkimiseen.  Omassa päässä ryvästäminen ja mallinnus menevät lähes samaan hyllyyn, mutta erotan ne juurikin tämän yksi vastaan useampi datalähde jaottelun avulla. Aihemallinnusta voidaan hyödyntää esimerkiksi viime luentoviikon aiheen luonnollisen kielen analyysin toteuttamisessa. Mikkonen (2021) esitteli luennollaan, kuinka luonnollista kieltä voidaan esikäsittelyn jälkeen analysoida vektorimallin avulla. Tässä mallissa sanat nimenomaisesti yhdistetään johonkin tiettyyn ryhmään tai 'aiheeseen', mikä on mielestäni erinomainen ja konkreettinen esimerkki kuvaamaan aihemallinnusta.

# Tällä luentoviikolla opin, mitä ohjaamaton oppiminen oikeastaan on ja kuinka se eroaa ohjatusta oppimisesta. Keskeinen oivallus tällä viikolla oli, että ohjaamaton oppiminen ei oikeastaan koskaan ole täysin ohjaamatonta, sillä dataa ja sen tuloksia manipuloidaan esimerkiksi klustereiden lukumäärää muuttamalla. Mielenkiintoista oli myös oppia mitä erilaisia metodeja ohjaamattomaan oppimiseen kuuluu ja, kuinka niitä voidaan Pythonilla käytännössä toteuttaa. Parantaisin luentoviikkoa siten, että poistaisin luennon työkirjasta demokoodit lähes kokonaan ja korvaisin nämä eri metodien konkreettisilla esimerkeillä tai prosessimalleilla. 

#                                                     

# ## Luentoviikko 7: Visuaalinen analytiikka 

# Opintojakson viimeisellä luentoviikolla tutustuttiin visuaaliseen analytiikkaan ja sen merkitykseen datatieteessä. Oppimispäiväkirjan kirjoittamisessa hyödynsin pelkästään luennon tallennetta. Kyseisellä luentoviikolla ei järjestetty demosessiota.

# Visuaalisella analytiikalla on datatieteessä kaksi eri roolia. Se on osana  eksploratiivista eli kartoittavaa analytiikkaa, sillä siinä katroitetaan lähtödataa tutkivalla otteella. Tämän lisäksi se on osana datan tutkimisen ja jalostamisen välivaiheiden ja tulosten kommunikointia loppukäyttäjälle. Visualisoinneista on usein taulukkomuotoiseen dataan verratuna helpompaa löytää ja tunnistaa yleispiirteitä, jolloin loppukäyttäjältä ei odoteta niin suurta tietämystä datan tulkinnasta. Kaikista yksinkertaisimpia visualisointeja voidaan esimerkiksi nähdä mainoksissa, joissa johinkin tuotteeseen liittyvää dataa on esitetty silmää miellyttävässä visuaalisessa muodossa. Tällöin asiakkaan on taustatiedoistaan huolimatta helppoa vetää johtopäätöksiä tuotteen laadusta. Mielestäni tässä kuitenkin tulee muistaa, että visualisointeja voidaan manipuloida niiden taustalla olevaa dataa jalostamalla sekä esittämällä käyttäjälle vain kyseisen visualisoitavan datan hyvät puolet.

# Visuaalinen analytiikka voidaan nähdä datan, visualisointien, mallien ja tietämyksen ympärillä pyörivänä prosessina. Visualisoinnin tavoitteena on eksploratiivisen analyysin näkökulmasta löytää oikeampi tutkimuskysymys oikean vastauksen löytämisen sijasta. Tällöin prosessissa muokataan lähtödataa siten, että siitä saadaan muodostettua mielekkäitä visualisointeja ja malleja, joista edelleen voidaan tehdä harkittuja tulkintoja, jolloin päästään soveltamisen kautta tietämykseen. Tämä prosessi voidaan liittää myös tiedon tasojen kolmioon, jossa prosessin lopputuote sijaitsee kolmion kärjessä tietämyksen kohdalla, minne ollaan edetty lähtien datasta kulkien informaation kautta soveltamisen myötä saavutetun tietämyksen luo. Tämä kyseinen pyramidimalli on minulle entuudestaan tuttu Tietojohtamisen perusteet -kurssilta. Kyseisellä kurssilla sivuttiin myös visualisointia, mutta erittäin pintapuolisesti eikä sen syvempään tarkoitukseen missään vaiheessa päästy. Luentomateriaaleissa esitettiin myös kyseisen peruskurssin tietoihin liittyvä väite, että visualisoinneilla päästään eroon ihmisiä nykypäivänä kuormittavasta tietotulvasta ja päästään käsiksi vain kaikista olennaisimpaan tietoon. Tämä on mielestäni aivan täydellinen kuvaus visualisointien tarkoituksesta datatieteessä, jossa tutkittavat datamäärät yleisesti ovat valtavia.

# Viimeisen viikon demokoodissa halusin keskittyä nimenomaan erilaisten visualisointien toteuttamiseen, sillä uskoin tämän hyödyttävän harjoitustyön visualisointien tekemisessä. Tässä datana toimi edelliseltä viikolta tuttu Inside AirBnB:n data Amsterdamista. Oppimispäiväkirjan viimeinen koodidemo on esitettynä alla.

#                                                

# In[66]:


#Piirretään kuvaaja asuntojen tyypeistä
import matplotlib.pyplot as figure

plt.figure(figsize=(20,4))
plt.title('Asunnon tyyppi')
df3['room_type'].value_counts().plot(kind='bar', color='Pink')


# In[73]:


# Muodostetaan ympyrädiagrammi asuntojen tyyppien määristä
from collections import Counter
pie_nb = Counter(df3.room_type)
nb_df = pd.DataFrame.from_dict(pie_nb, orient='index').sort_values(by=0)
nb_df.columns = ['room_type']
nb_df.plot.pie(y='room_type', colormap='Reds_r', figsize=(8.5,8.5), fontsize=20, autopct='%.2f', legend=False)


#                                                      

# Visualisointeja tehdessä on aina tärkeää miettiä, kenelle kyseistä visualisointia tehdään, sillä se vaikuttaa suoraan siihen millainen visualisointi on mielekäs ja tarkoituksenmukainen. Visualisoinnin tulee olla selkeä ja helposti tulkittavissa. Niissä ei esimerkiksi kannata esittää turhan suurta määrää tietoa sekä niiden värityksessä on hyvä ottaa huomioon esimerkiksi puna-vihervärisokeat. Yleisesti suositellaan välttämää piirakkadiagrammeja, mutta mielestäni yläpuolella esittämäni diagrammi kuvastaa erinomaisesti haluamaani asiaa. Näkisin siis käytettävän visualisoinnin tyypin olevan myös riippuvainen tutkittavasta datasta ja tutkimuskysymyksestä. Itse olen aiemmissa opinnoissani tehnyt yksinkertaisia visualisointeja excelillä sekä lisäksi Liiketoimintatiedon hallinta -kurssilla pääsin kokeilemaan Power BI:tä, joka vaikutti erittäin toimivalta työkalulta tähän tarpeeseen ja sitä haluaisin opetella lisää vielä tulevaisuudessa.

# Kyseiseltä luoentoviikolta käteen jäi kasa taustateoriaa visualisoinneista, tietämystä siitä, kuinka visualisoinnit kuuluvat osana datatieteeseen sekä lisäksi opin kuinka visualisointeja voidaan tehdä Pythonia hyväksikäyttäen. Parannusehdotuksena luentoviikolle ehdottaisin vieläkin useampien eri visualisointityökalujen esittelyä, sillä mielestäni viikon aihealue on melko selkeä eikä teoriaa ole tarve käydä läpi niin yksityiskohtaisesti.

# Näin viimeisen oppimispäiväkirjan luvun kunniaksi pohditaan vielä, kuinka paljon olenkaan opintojakson aikana oppinut. Ensimmäisenä tavoitteenani opintojaksolle lähdettäessä oli oppia ymmärtämään paremmin mitä kaikkia aihealueita datatieteeseen sisältyy ja kuinka nämä suhtautuvat toisiinsa nähden. Näen onnistuneeni tässä tavoitteessa hyvin, sillä kurssin luentorunko tuki tavoitteen toteutumista ja tietämys kasaantui opintojakson edetessä, vaikka jossakin vaiheessa jokin osa-alue saattoikin alkuun jäädä hieman hämärän peittoon. Seuraava oppimistavoitteeni oli selvittää, kuinka datatiede näkyy yritysten toiminnassa. Tämä mielestäni avautui hieman ja etenkin vierailuluento avasi silmiä eri mahdollisuuksien suhteen, mutta kokonaisuudessaan en näe oppineeni asiasta kurssin aikana kovinkaan konkreettisesti. Viimeinen oppimistavoitteeni oli kehittyä ohjelmoijana, mikä mielestäni onnistui varsin hyvin, sillä sekä oppimispäiväkirjan että harjoitustyön parissa puuhastellessa oppi uutta suorastaan huomaamatta. Kokonaisuutena kurssi oli erittäin opettavainen ja herätti vieläkin suuremman kiinnostuksen datatiedettä kohtaan.   

# ## Lähteet 

# Avoin data. (2018). Väistötiloissa opiskelevat peruskoululaiset syyslukukaudella 2018. Saatavilla www-muodossa osoitteessa: https://www.avoindata.fi/data/fi/dataset/vaistotiloissa-opiskelevat-peruskoululaiset-syksy-2018/resource/11e1d4b7-6a06-4c8a-a504-6ccede3a786d (Viitattu 11.5.2022)
# 
# Avoin data. (2022a). Kelan etuuksien saajat ja maksetut etuudet. Saatavilla www-muodossa osoitteessa: https://www.avoindata.fi/data/fi/dataset/kelan-etuuksien-saajat-ja-maksetut-etuudet/resource/a3be1d46-1c33-4a59-84d4-66a348b1551c (Viitattu 12.5.2022)
# 
# Avoin data. (2022b). Maksetut takuueläkkeet. Saatavilla www-muodossa osoitteessa: https://www.avoindata.fi/data/fi/dataset/maksetut-takuuelakkeet/resource/bc9a6486-3f0c-40cc-8016-8b7e803075b8 (Viitattu 12.5.2022)
# 
# Gigantti. (n.d.). Weber Genesis II E310 kaasugrilli 61010169 (musta). Saatavilla www-muodossa osoitteessa: https://www.gigantti.fi/product/kodin-pienkoneet/grillit/kaasugrillit/weber-genesis-ii-e310-kaasugrilli-61010169-musta/17523 (Viitattu 12.5.2022)
# 
# Inside AirBnB. (n.d.). Get the Data. Amsterdam. Saatavilla www-muodossa osoitteessa: http://insideairbnb.com/get-the-data/ (Viitattu 12.5.2022)
# 
# Mikkonen, T. (2021). Vierailuluento: Luonnollisen kielen analyysi. Luento Tampereen Yliopistossa keväällä 2021. Luentomateriaalit saatavilla www-muodossa osoitteessa: https://github.com/MikkonenTS/Futurice-Joda/blob/main/FuturiceJODA.ipynb
# 
# School of data science Amsterdam. (2017) What skills do I need to become a modern Data Scientist? Saatavilla www-muodossa osoitteessa: https://www.schoolofdatascience.amsterdam/news/skills-need-become-modern-data-scientist/ (Viitattu 12.5.2022)
